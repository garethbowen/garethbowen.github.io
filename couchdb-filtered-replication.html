<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Scaling CouchDB to 100 million patients - Gareth Bowen</title>
    <link rel="stylesheet" href="./style.css">
    <link rel="apple-touch-icon" sizes="180x180" href="./logo/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./logo/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./logo/favicon-16x16.png">
    <link rel="manifest" href="./site.webmanifest">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Overlock+SC&display=swap" rel="stylesheet">
  </head>
  <body>
    <div>
      <div class="contact">
        <div class="logo">
          <a href="https://gareth.nz"><img src="./logo/logo-white.svg" alt="Gareth Bowen" width="50" /></a>
        </div>
        <div class="social">
          <ul>
            <li><a href="mailto:gareth@bowenwebdesign.co.nz"><img src="./icons/icons8-email-64.png" width="32" height="32" alt="Email Gareth Bowen"/></a></li>
            <li><a href="https://www.linkedin.com/in/garethjbowen/"><img src="./icons/icons8-linkedin-64.png" width="32" height="32" alt="View Gareth Bowen LinkedIn profile"/></a></li>
            <li><a href="https://github.com/garethbowen"><img src="./icons/icons8-github-64.png" width="32" height="32" alt="See Gareth's software development at GitHub"/></a></li>
            <li><a href="https://www.strava.com/athletes/13372622"><img src="./icons/icons8-strava-64.png" width="32" height="32" alt="Follow Gareth on Strava"/></a></li>
            <li><a href="https://mastodon.nz/@garethbowen"><img src="./icons/icons8-mastodon-64.png" width="32" height="32" alt="Lurking on mastodon"/></a></li>
          </ul>
        </div>
      </div>
    </div>
    <div class="banner">
      <div>
        <h1>Gareth Bowen</h1>
      </div>
    </div>
    <div class="section-title">
      <div>
        <h2>Scaling CouchDB to 100 million patients</h2>
        <p>Supporting users using offline first PWAs with filtered replication at scale.</p>
        <p>20 March 2025</p>
      </div>
    </div>
    <div class="splash">
      <div>
        <img src="./photos/scaling-couch.jpg" alt="Scaling CouchDB banner">
        <p class="caption">Generated by deepai.org</p>
      </div>
    </div>
    <div class="section-title">
      <div>
        <a id="what-is-filtered-replication" href="#what-is-filtered-replication"><h3>What is filtered replication?</h3></a>
      </div>
    </div>
    <div>
      <div>
        <p>Replication is the process by which CouchDB syncs data between two databases so at the end their contents are identical. Sometimes, however, you want one database to only have a subset of the documents, for example, when users don't have permission to see all docs, or when replicating to a phone which doesn't have the space to store the entire database. This is when filtered replication is needed.</p>
        <p>The filter is essentially just a function which returns true or false for a given document. The logic of this function is custom to your application based on your permissions model, so it needs to be entirely configurable.</p>
        <p>In CouchDB, the replication algorithm gets a list of changes in chronological order since the last time you synced. These are filtered one by one and the appropriate changes are returned. The changed docs are then fetched in batches from the source db and stored in the target db.</p>
        <p>In the <a href="https://github.com/medic/cht-core">Community Health Toolkit</a> we use filtered replication to sync docs from the server to a client side PouchDB instance. PouchDB natively supports the <a href="https://docs.couchdb.org/en/stable/replication/protocol.html">CouchDB replication protocol</a> so it works out of the box. Initially the CHT was only used on deployments with hundreds of users, but as the scale increased we needed to constantly redevelop the way we performed filtered replication.</p>
        <p>This is the story of that journey and what I learned along the way.</p>
      </div>
    </div>
    <div class="section-title">
      <div>
        <a id="attempts" href="#attempts"><h3>Attempts</h3></a>
      </div>
    </div>
    <div>
      <div>
        <a id="native-functions" href="#native-functions"><h4>v1: native functions</h4></a>
        <em>0-10 users</em>
        <p>The naive implementation is to provide a <a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html#filter-functions">Javascript function as a filter</a>. With this approach, every doc must be fetched from disk and then serialized into JSON so it can be passed in to the filter function to determine if it should be returned or not.</p>
        <pre>function(doc, req) {
  if (doc.type === 'shared') {
    // all users can access shared docs
    return true;
  }
  if (doc.owner === req.userCtx.name) {
    // users can access their own docs
    return true;
  }
  return false;
}</pre>
        <p>Adding users means sync requests and more docs to fetch, so it has O(N2) performance. Even with only one user filtering a lot of documents is incredibly slow, and if around 10 users synced at the same time the requests would start to queue up and time out. Suffice to say, this was abandoned very early on.</p>
        <p>One evolutionary deadend was rewriting the function in erlang. This gave roughly 3x performance improvement, which is great, but the scalability is still unacceptable.</p>
        
        <a id="filter-by-doc-id" href="#filter-by-doc-id"><h4>v2: filter by doc ID</h4></a>
        <em>10-100 users</em>
        <p>An alternative to using a filter function is providing a list of document IDs. This performs much better because the algorithm can determine which changes are relevant without fetching the doc from the db. This was implemented by finding every ID in the target db we could determine if any docs had changed in a way that scaled well and including the list of known IDs in the request to the server. With thousands of IDs the request started to get chunky. Furthermore the native solution had no way to fetch new documents that the client didn't know about yet.</p>
                
        <a id="view-lookups" href="#view-lookups"><h4>v3: view lookups</h4></a>
        <em>100-1000 users</em>
        <p>At this point we had exhausted all possibilities of the natively supported CouchDB functionality and we took the leap to augment the process. We wanted to maintain protocol compatibility so at least the client implementation wouldn't need to change. With this attempt the server intercepted the replication request and looked up the list of IDs the user was permitted to view before proxying the request to CouchDB for changes that match those IDs. The IDs were looked up using CouchDB map/reduce views performs well and does the caching for you so repeated lookups are fast so we thought we were done.</p>
        <p>But as we kept scaling we found the next bottleneck...</p>
                
        <a id="custom-initial-replication" href="#custom-initial-replication"><h4>v4: custom initial replication</h4></a>
        <em>1k-5k users</em>
        <p>The majority of CHT users only see documents they have created plus a small subset of docs that everyone can view. This means the changes feed is very sparse for each user, ie: if you have a thousand  users, then each user is able to access only ~0.1% of docs, meaning you have to scroll through a lot of changes before finding even one that passes the filter. Therefore when a user first logs in the server has to scan through every change that has ever been made before responding to the request. After a few years of operation this means the changes request starts hanging and eventually times out, at which point the client reissues the request from the start again. When onboarding a new cohort of users for the first time this can lock up the server completely.</p>
        <p>Given that the worst case was with users logging in for the first time to a server with millions of changes. This is a special case because the client db is empty so you don't need to worry about document deletions or merge conflicts. Once again we found ourselves having to diverge from the native implementation. Instead of using the replication protocol we implemented a custom API to get the latest version of every doc the user can access. Once that completes the native replication can begin from then on.</p>
        <p>This solves the worse case scenario but there are still edge cases. One example is users who don't connect often and end up with a lot of changes to crawl through again. Another example is if a migration updates many documents in one go causing millions of changes to be added to the feed in one go.</p>
        
        <a id="custom-ongoing-replication" href="#custom-ongoing-replication"><h4>v5: custom ongoing replication</h4></a>
        <em>5k-100k users</em>
        <p>Now that we had solved the initial replication problem we decided to use this pattern for ongoing replication as well. The client now periodically calls a custom API that returns the latest version number of all docs the user is allowed to access. Then the client finds which of these versions differ from the version in the local DB and fetches and merges these in batches. This completely removes the need for querying the changes feed which was the ultimate bottleneck for large scale deployments.</p>
        <p>This version finally supports tens of thousands of users updating millions of documents every month in production.</p>
      </div>
    </div>
    <div class="section-title">
      <div>
        <a id="other-ideas" href="#other-ideas"><h3>Other ideas</h3></a>
      </div>
    </div>
    <div>
      <div>
        <a id="db-per-user" href="#db-per-user"><h4>DB per user</h4></a>
        <p>CouchDB supports <a href="https://docs.couchdb.org/en/stable/config/couch-peruser.html">db-per-user pattern</a> where each user has their own DB instance. This removes the need to use filtered replication because the source and target DB have the same document access. For the CHT this would not have worked well because many docs were shared with other users, for example, all the user's docs should be accessible to their supervisor. If user-per-db were used, then the filtering would still have to be applied between every db to every other db on the server, which has the same scalability issue as more users are added.</p>

        <a id="per-document-access-control" href="#per-document-access-control"><h4>Per document access control</h4></a>
        <p>One as yet unreleased feature is <a href="https://github.com/apache/couchdb/issues/1524">per document access control</a> which adds permissions to the doc for natively supporting doc level access. One downside is it stores access in a field on the doc which means if the permissions are changed then multiple documents will need to be updated to reflect the new access, thus adding a large number of lines to the changes feed. Watch this space.</p>

        <a id="throwing-hardware-at-it" href="#throwing-hardware-at-it"><h4>Throwing hardware at it</h4></a>
        <p>One of the first attempts at solving replication throughput was increasing the number of cores available to CouchDB. Initially this was the latest and greatest v2, which maxed out at one CPU per data shard which defaults to 8, so there's no improvement past about 10 cores. It's difficult to increase the number of shards with existing data, so you definitely want to think carefully about how many shards you think you'll need before getting started. The latest CouchDB v3 improves the multi-threading to make better use of additional cores. It also made it easier to increase the number of shards through resharding.</p>
        <p>The next idea was to increase the memory as instances were using all available RAM, but CouchDB kept consuming 100% of the RAM that was given. For a database this is ideal, because we needed it to cache as much as it was allowed. This made no noticeable improvement to overall scalability, because the problem was CPU and IO bound.</p>
        <p>Clustering works extremely well in CouchDB for example when testing a three node cluster we measured a 3x performance improvement. Much like with shards it's difficult to change the number of nodes in the cluster once in production, so it pays to work out what scale you'll need before going live. The downside is that clustering significantly increases deployment complexity. Additionally to get reliable uptime CouchDB uses replicas of the data on different nodes so if one node is unavailable the server can still respond which doubles the double disk usage.</p>
        <p>Using more shards and clustered nodes multiplied the performance and scalability of replication, and were complementary with the algorithmic changes above.</p>

        <a id="replacing-couchdb" href="#replacing-couchdb"><h4>Replacing CouchDB</h4></a>
        <p>Every time a new version was discussed we evaluated whether CouchDB was the right DB for our use case. Migrating to a different DB is always disruptive. This is especially true if using protocols which couple the server and client side DBs, which mean you either need to replace both at the same time, or reimplement the protocol on top of the new DB. It's also very difficult to test performance and scalability before you've done the work, so it's hard to know if the effort will pay off or not.</p>
        <p>With v5 the algorithm is no longer using the CouchDB replication protocol which makes it possible to change the server DB without changing the client implementation at all. This was an intentional secondary benefit to switching to using a custom algorithm.</p>
      </div>
    </div>
    <div class="section-title">
      <div>
        <a id="conclusion" href="#conclusion"><h3>Conclusion</h3></a>
      </div>
    </div>
    <div>
      <div>
        <p>These iterations took the best part of a decade of iteration and experimentation - filtered replication at scale is difficult. In general I avoid reinventing the wheel, however in this case, it was identified that the requirements were for a high degree of configurability of access rules while maintaining scalability. For a core feature of a product you should take the time to develop a custom solution that you can have full control over.</p>
      </div>
    </div>
    <div class="footer">
      <div>
        &copy; Gareth Bowen
      </div>
    </div>
  </body>
</html>
